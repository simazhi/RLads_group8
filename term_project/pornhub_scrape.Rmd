---
title: "pornhub_scrape"
author: "Thomas"
date: "16/12/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Packages
```{r}
library(rvest)
library(tidyverse)
library(stringr)
library(robotstxt) #this checks if you srape a certain website, useful for debugging
```

# Scraping
Can I scrape the website?

```{r}
robotstxt::paths_allowed("https://www.pornhub.com")
robotstxt::robotstxt("https://www.pornhub.com")$crawl_delay %>% tbl_df()
```

# Plan for retrieving information

![workflow](https://github.com/simazhi/RLads_group8/term_project/images/image_preview.png)


# Test case

## Finding out which links we should use
lesbian + most viewed + 1000 first

lesbian is category 27
mostviewd is mv

page 1: 1-32 => 32 per page
page 2: 45-88 => 43 per page
page 3: 89-132 => 43 per page
...
so to get at 1000 we need to get 22 + 1 (for the first page) = to number 23

```{r}
(1000-32) %/% 43
```

The link then looks like this:
```https://www.pornhub.com/  video?  c=X   &  o=Y  &  page=Z```
with X = 27, Y = mv, Z = 1 to 23

# Getting the links in one function

```{r}
resultspages <- 1:23  ##Change this to 23 for the complete set
url_base <- "https://www.pornhub.com/video?c=27&o=mv&page=%d"


link_check <- map(resultspages, function(i){
  
  cat(".")
  
  links <- read_html(sprintf(url_base, i)) %>%
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "title", " " ))]//a') %>%
    html_attr('href') %>%
    str_replace_all("/(view.+)", "https://www.pornhub.com/\\1") %>%#add website domain
    as.tibble() %>%
    filter(grepl("^https", value)) %>%
    pull(value) #turns variable in tibble into vector
    
    return(links)
  
})
link_check <- unlist(link_check)
#link_check <- link_check[1:5]

#link_check
```

```{r}
write.table(link_check, file = "links.txt")
```

### subset of links
```{r}
#link_check <- link_check[1:2]
#link_check
```


# Scraping the webpages


## Video pages (1 page -> 1 kind of element)
```{r echo = FALSE}
videopages <- map_df(link_check,
                function(url){
                  tryCatch({
                  
                  cat(paste("Doing", url, "\n"))
                  
                  page <- url %>%
                    read_html()
                  

                  #numerical data
                  ##views
                  views <- page %>%  
                    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "count", " " ))]') %>%
                    html_text() %>%
                    str_replace_all(",", "") %>%
                    as.numeric()
                  
                  ##percentage
                  percentage <- page %>%
                    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "percent", " " ))]') %>%
                    html_text() %>%
                    str_replace_all("%", "") %>%
                    as.numeric()
                    
                  ##upvotes
                  upvotes <- page %>%
                    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "votesUp", " " ))]') %>%
                    html_text() %>%
                    as.numeric()
                  
                  ##downvotes
                  downvotes <- page %>%
                    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "votesDown", " " ))]') %>%
                    html_text() %>%
                    as.numeric()
                  
                  #nominal data
                  ##categories
                  categories <- page %>%
                    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "categoriesWrapper", " " ))]') %>%
                    html_text() %>%
                    str_replace_all("\n|\t|\\+ Suggest", "") %>%
                    str_replace_all("^Categories:", "")  %>%
                    str_replace_all("^\\s", "")
                    as.character()

                  ##data frame
                  df <- data.frame(
                    url = unlist(url),
                    views = views,
                    upvotes = upvotes,
                    downvotes = downvotes,
                    percentage = percentage,
                    categories = categories
                  )
                  
                  
                  
                  }, error = function(e) {
                    Sys.sleep(2)
                    e
                  }
                  ) 
                  
                })
#View(teksten)

videopages

write_csv(videopages, "videopages.csv")
```


## Comments (1 page -> many comments)

```{r}
usercomments <- map_df(link_check,
                function(url){
                  tryCatch({
                  
                  cat(paste("Doing", url, "\n"))                  
                  
                  page <- url %>%
                    read_html()
                  
                  #comments
                  comments <- page %>%
                    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "commentMessage", " " ))]') %>%
                    html_text() %>%
                    str_replace_all("\t|\n| Reply|â€¢", "") %>%
                    str_replace_all("(\\d+)$", " \\1") %>%
                    str_replace_all("\\[\\[commentMessage\\]\\] 0", "")
                  
                  comments <- comments[comments != ""]

                  ##commenters
                  commenters <- page %>%
                    html_nodes(xpath = '//*[(@id = "cmtContent")]//*[contains(concat( " ", @class, " " ), concat( " ", "usernameLink", " " ))]') %>%
                    html_text()                
                  
                  l <- length(commenters)
                  
                  origin <- rep(url, l)
                  
                  #Data frame
                  df <- data.frame(
                    origin = unlist(origin),
                    commenters = commenters,
                    comments = comments
                  )
                  
                  
                  
                  }, error = function(e) {
                    Sys.sleep(2)
                    e
                  }
                  )
  
                })
#View(teksten)
usercomments

write_csv(usercomments, "usercomments.csv")
```



## Get user data

### Links to user profiles
```{r user.list}
userlist <- usercomments %>%
  select(commenters) %>%
  distinct() %>%
  pull() %>%
  str_replace_all("\\s", "%20")

#userlist

url_base <- "https://www.pornhub.com/users/%s"


userprofile <- map(userlist, function(i){
  links <- sprintf(url_base, i) 
  return(links)
})

userprofile <- unlist(userprofile)
#userprofile
```

### Subselection of profiles
```{r}
#userprofile <- userprofile[1]
#userprofile
```



```{r}
vars <- read_html(userprofile) %>%
  html_nodes(xpath = '//*[@id="profileInformation"]/div/div[2]/dl') %>%
                    html_text() %>% 
                    str_replace_all("\t+", "") %>%
                    str_replace_all("\n", " ")

vars

interest <- vars %>%
                    str_extract("Interested\\sIn:\\s[A-Z][a-z]+\\s") %>%
                    str_replace_all("Interested\\sIn:\\s", "") %>%
                    str_replace_all("\\s", "")

interest
```


### Data on userprofiles
```{r}

userdata <- map_df(userprofile,
                function(url){
                  tryCatch({
                  
                  cat(paste("Doing", url, "\n"))                  
                  
                  page <- url %>%
                    read_html()
                  
                  user <- url %>%
                    str_replace_all("^https://www.pornhub.com/users/", "")

                  #intro    but do we want this?
                  #intro <- page %>%
                  #  html_nodes('#profileInformation > div > div.profileInfo > dl > p') %>%
                  #  html_text() 
                  
                  
                  #variables
                  vars <- page %>%
                    html_nodes(xpath = '//*[@id="profileInformation"]/div/div[2]/dl') %>%
                    html_text() %>% 
                    str_replace_all("\t+", "") %>%
                    str_replace_all("\n", " ")
                  
                  ##Gender
                  gender <- vars %>%
                    str_extract("Gender:.+Age:") %>%
                    str_replace_all("Gender:|Age:", "") %>%
                    str_replace_all("\\s", "")
                  
                  ##Age
                  age <- vars %>%
                    str_extract("Age: \\d+") %>%
                    str_replace_all("Age:", "") %>%
                    str_replace_all("\\s", "")
                  
                  ##Last Login
                  lastlogin <- vars %>%
                    str_extract("Last Login:.+ago\\s[A-Z]") %>%
                    str_replace_all("Last Login:\\s|\\s[A-Z]", "")
                  
                  ##Relationship Status
                  relstatus <- vars %>%
                    str_extract("Status:.+Interested") %>%
                    str_replace_all("^.+:|Interested", "") %>%
                    str_replace_all("\\s", "")
                  
                  ##Interested in
                  interest <- vars %>%
                    str_extract("Interested\\sIn:\\s[A-Z][a-z]+\\s") %>%
                    str_replace_all("Interested\\sIn:\\s", "") %>%
                    str_replace_all("\\s", "")
                  
                  ##Country
                  country <- vars %>%
                    str_extract("Country:\\s[A-Za-z]+(\\s[A-Za-z]+)?") %>%
                    str_replace_all("Country:\\s", "") %>%
                    str_replace_all("\\s", "_")
                  
                  ##City
                  city <- vars %>%
                    str_extract("City:.+Country:") %>%
                    str_replace_all("City:\\s|\\sCountry:", "") %>%
                    str_replace_all("\\s", "_")
                  
                  #data frame
                  df <- data.frame(
                    user = user,
                    gender = gender,
                    interest = interest,
                    age = age,
                    lastlogin = lastlogin,
                    relstatus = relstatus,
                    country = country,
                    city = city
                    )
                  
                  }, error = function(e) {
                    Sys.sleep(2)
                    e
                  }
                  )

                })
#View(teksten)
userdata

write_csv(userdata, "userdata.csv")
```








# To read back in use read_csv
```{r}
#a <- read_csv("usercomments.csv")
#a
```



# Problems

* Should we split categories from character vector into different values? (tidy data)
* Should the data be tidieR?
* Do we need the About intro on the profile page? Not everybody has one.






```{r}
for (i in seq_along(links)) {
  if (!(links[i] %in% names(d))) {
    cat(paste("Doing", links[i], "..."))
    ok <- FALSE
    counter <- 0
    while (ok == FALSE & counter <= 5) {
      counter <- counter + 1
      out <- tryCatch({                  
                  scrape_test(links[i])
                },
                error = function(e) {
                  Sys.sleep(2)
                  
                }
              )
      if ("error" %in% class(out)) {
        cat(".")
      } else {
        ok <- TRUE
        cat(" Done.")
      }
    }
    cat("\n")
    d[[i]] <- out
    names(d)[i] <- links[i]
  }
} 
```






---
# For only one variable

## Getting the links
```{r}
# set webpage 
page <- read_html("https://www.pornhub.com/video?c=27&o=mv&page=1")

link <- page %>%
  html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "title", " " ))]//a') %>%
  html_attr('href') 
## because I used the xpath notation I am sure that I'm grabbing the right things, so the steps below are unnecessary this time


#link

link_check <- link %>%
  str_replace_all("/(view.+)", "https://www.pornhub.com/\\1") %>%#add website domain
  as.tibble() %>%
  filter(grepl("^https", value)) %>%
  pull(value) #turns variable in tibble into vector

link_check

```


```{r}
testlink <- "https://www.pornhub.com/view_video.php?viewkey=ph59395b73b70ac"
link_check <- c("https://www.pornhub.com/view_video.php?viewkey=ph5681abe10ceed", "https://www.pornhub.com/view_video.php?viewkey=ph5a2b529d4e452")
a <- read_html(testlink)

b <- a %>%
                    html_nodes(xpath = '//*[(@id = "cmtContent")]//*[contains(concat( " ", @class, " " ), concat( " ", "usernameLink", " " ))]') %>%
                    html_text()

c <- a %>%
                
                    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "commentMessage", " " ))]') %>%
                    html_text() %>%
                    str_replace_all("\t|\n| Reply|â€¢", "") %>%
                    str_replace_all("(\\d+)$", " \\1") %>%
                    str_replace_all("\\[\\[commentMessage\\]\\] 0", "")
  
c
c[c != ""]

d <- length(c)
d


e <- rep(testlink, d)
e
```
